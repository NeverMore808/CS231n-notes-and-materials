内容列表：

- 简介
- 简单表达式和理解梯度
- 复合表达式，链式法则，反向传播
- 直观理解反向传播
- 模块：Sigmoid例子
- 反向传播实践：分段计算
- 回传流中的模式
- 用户向量化操作的梯度
- 小结

## 简介

**目标**：本节将帮助读者对**神经网络**形成直观而专业的理解，首先了解神经网络的一般架构，之后介绍其中重要的一环——利用**反向传播**计算梯度。反向传播是利用**链式法则**递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常**关键**。

**问题陈述**：这节的核心问题是：给定函数![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) ，其中![[公式]](https://www.zhihu.com/equation?tex=x)是输入数据的向量，需要计算函数![[公式]](https://www.zhihu.com/equation?tex=f)关于![[公式]](https://www.zhihu.com/equation?tex=x)的梯度，也就是![[公式]](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29)。

**目标**：之所以关注上述问题，是因为在神经网络中![[公式]](https://www.zhihu.com/equation?tex=f)对应的是损失函数（![[公式]](https://www.zhihu.com/equation?tex=L)），输入![[公式]](https://www.zhihu.com/equation?tex=x)里面包含训练数据和神经网络的权重。如果用随机探测，不仅不准确而且时间过长（但是通常可以在第一步用随即探测检查反向传播的正确性）。

举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据![[公式]](https://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29%2Ci%3D1...N)、权重![[公式]](https://www.zhihu.com/equation?tex=W)和偏差![[公式]](https://www.zhihu.com/equation?tex=b)。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据![[公式]](https://www.zhihu.com/equation?tex=x_i) 上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如![[公式]](https://www.zhihu.com/equation?tex=W%2Cb)）的梯度。然而![[公式]](https://www.zhihu.com/equation?tex=x_i%0A) 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。



## 简单的Neural Networks

**线性**方程：$f=Wx$

**两层**的神经网络：$f=W_2max(0,W_1x),x\in R^D,W_! \in R^{H \times D},W_2 \in R^{C \times H}$

![](images/33.png)

可以理解为10类又细分为100类，从而更好地处理每一类中图片不同情况的问题。$W_1$对图片处理，$W_2$对h的分类进行加权整合



**常用的激活函数**

![](images/34.png)



**对于Neural Networks结构的表述**

![](images/35.png)

```python
# forward pass of a 3-layer neral network:
f = lambda x:1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3,1) # radom input vector of three numbers (3*1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4*1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4*1)
out = np.dot(W3, h2) + b3 # output neuron (1*1)
```

我们通常称为三层神经网络（3-layer Neural Net），也称为 2-hidden-layer Neural Net



**生物学类比**

![](images/36.png)

至于“神经网络”，因为它与神经元有着极大的相似之处，树突相当于输入的数据，细胞体对输入数据的处理相当于f，轴突向下的传导相当于output；激活函数和神经元的发放率也有着相似的模式



## 反向传播算法计算梯度

#### 简单的例子

首先要知道链式法则，反向传播的过程像是利用链式法则逐层求导的过程

![](images/37.png)

$f(x,y,z)=(x+y)z,\quad let x=-2,y=-5,z=-4$

$q=x+y,\frac{\delta q}{\delta x}=1,\frac{\delta q}{\delta y}=1 \quad f=qz,\frac{\delta f}{\delta q}=z,\frac{\delta f}{\delta z}=q$

$\frac{\delta f}{\delta x}=\frac{\delta f}{\delta q} \frac{\delta q}{\delta x}=z=-4 \quad \frac{\delta f}{\delta y}=\frac{\delta f}{\delta q} \frac{\delta q}{\delta y}=z=-4$

$\frac{\delta f}{\delta z}=q=x+y=3$

其实我们并不关心中间节点q的梯度，它对整个计算并没有意义，而是我们利用链式法则的工具

取最大值操作也是常常使用的：
![[公式]](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3Dmax%28x%2Cy%29+%5Cto+%5Cfrac+%7Bdf%7D%7Bdx%7D%3D1+%28x%3E%3Dy%29+%5Cquad%5Cfrac+%7Bdf%7D%7Bdy%7D%3D1+%28y%3E%3Dx%29)

上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若![[公式]](https://www.zhihu.com/equation?tex=x%3D4%2Cy%3D2)，那么max是4，所以函数对于![[公式]](https://www.zhihu.com/equation?tex=y)就不敏感。也就是说，在![[公式]](https://www.zhihu.com/equation?tex=y)上增加![[公式]](https://www.zhihu.com/equation?tex=h)，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给![[公式]](https://www.zhihu.com/equation?tex=y)增加一个很大的量，比如大于2，那么函数![[公式]](https://www.zhihu.com/equation?tex=f)的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：![[公式]](https://www.zhihu.com/equation?tex=lim_%7Bh%5Cto+0%7D)。



#### 模块化：以sigmoid为例

$f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$

![](images/38.png)

$f(x)=1/x \rightarrow \frac{df}{dx}=-1/x^2 \quad f(x)=x+1 \rightarrow \frac{df}{dx}=1$

$f(x)=e^x \rightarrow \frac{df}{dx}=e^x \quad f(x)=wx \rightarrow \frac{df}{dx}=w$

代入计算，可得所有边上的梯度值，并且发现其中的sigmoid可以将$\sigma(x)=\frac{1}{1+e^{-x}}$模块化为一个“黑箱子”，$\frac{d\sigma(x)}{dx}=(1-\sigma (x))\sigma (x)$

![](images/39.png)

通常，我们也把一个节点叫做gate，常用的有+、*、max、copy

因为是从结果向前计算，所以正向计算输出，反向传播求梯度



该神经元反向传播的代码实现如下：

```python
w = [2,-3,-3] # 假设一些随机数据和权重
x = [-1, -2]

# 前向传播
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid函数

# 对神经元反向传播
ddot = (1 - f) * f # 点积变量的梯度, 使用sigmoid函数求导
dx = [w[0] * ddot, w[1] * ddot] # 回传到x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # 回传到w
# 完成！得到输入的梯度
```



**实现提示：分段反向传播**。

上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量**dot**，它装着**w**和**x**的点乘结果。在反向传播的时，就可以（反向地）计算出装着**w**和**x**等的梯度的对应的变量（比如**ddot**，**dx**和**dw**）。



#### 反向传播的直观理解

反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。

> 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。

下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以**x**和**y**的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果**x，y减小**（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。

因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。



#### 高维扩展

![](images/40.png)

对于高维（m）的输入和高维（n）的输出，梯度是雅可比矩阵（m*n）

在上图的例子中，由于$y_i$只与$x_i$有关，所以Jacobian矩阵是一个对角矩阵，从而我们不需要将m*n的空间来储存，可以只存对角矩阵的主对角线，从而节省空间



#### 注意

**对前向传播变量进行缓存**：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。

**在不同分支的梯度要相加**：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用**+=**而不是**=**来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的*多元链式法则*，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。



## 小结

- 直观地认识了**神经网络**的基本架构和函数构成，就是通过一层一层的计算处理数据，类似于将原始的图片分成不同的小类，再对小类加权形成大类，相当于对一个类形成了不同的模板
- 对**梯度**的含义有了直观理解，知道了梯度是如何在网络中**反向传播**的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。
- 讨论了**模块化**在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。

在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，这就是本课程最困难的部分了，ConvNets相比只是向前走了一小步。